package tokenizer

import (
	"context"
	"reflect"
	"testing"
)

func TestRuleBoundaryTokenizer_SimpleText(t *testing.T) {
	tokenizer := NewRuleBoundaryTokenizer(nil) // ä½¿ç”¨é»˜è®¤é…ç½®
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	tests := []struct {
		name     string
		input    string
		expected []string
	}{
		{
			name:  "ä¸­æ–‡ç®€å•å¥å­",
			input: "è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¥å­ã€‚è¿™æ˜¯ç¬¬äºŒä¸ªå¥å­ã€‚",
			expected: []string{
				"è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¥å­ã€‚",
				"è¿™æ˜¯ç¬¬äºŒä¸ªå¥å­ã€‚",
			},
		},
		{
			name:  "è‹±æ–‡ç®€å•å¥å­",
			input: "This is a simple sentence. This is the second sentence.",
			expected: []string{
				"This is a simple sentence.",
				"This is the second sentence.",
			},
		},
		{
			name:  "æ··åˆè¯­è¨€",
			input: "è¿™æ˜¯ä¸­æ–‡ã€‚This is English. è¿™æ˜¯æ··åˆChinese and ä¸­æ–‡ï¼",
			expected: []string{
				"è¿™æ˜¯ä¸­æ–‡ã€‚",
				"This is English.",
				"è¿™æ˜¯æ··åˆChinese and ä¸­æ–‡ï¼",
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tokenizer.Feed(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Feed() = %v, want %v", got, tt.expected)
			}
		})
	}
}

func TestRuleBoundaryTokenizer_URLs(t *testing.T) {
	tokenizer := NewRuleBoundaryTokenizer(nil)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	tests := []struct {
		name     string
		input    string
		expected []string
	}{
		{
			name:  "ç®€å•URL",
			input: "è¯·è®¿é—® https://example.com/page.html è·å–æ›´å¤šä¿¡æ¯ã€‚ä¸‹ä¸€å¥å¼€å§‹ã€‚",
			expected: []string{
				"è¯·è®¿é—® https://example.com/page.html è·å–æ›´å¤šä¿¡æ¯ã€‚",
				"ä¸‹ä¸€å¥å¼€å§‹ã€‚",
			},
		},
		{
			name:  "å¤šä¸ªURL",
			input: "ç¬¬ä¸€ä¸ªé“¾æ¥ http://site1.comï¼Œç¬¬äºŒä¸ªé“¾æ¥ https://site2.comã€‚",
			expected: []string{
				"ç¬¬ä¸€ä¸ªé“¾æ¥ http://site1.comï¼Œç¬¬äºŒä¸ªé“¾æ¥ https://site2.comã€‚",
			},
		},
		{
			name:  "å¸¦å‚æ•°çš„URL",
			input: "è®¿é—® https://example.com/search?q=test&page=1 æŸ¥çœ‹ç»“æœã€‚",
			expected: []string{
				"è®¿é—® https://example.com/search?q=test&page=1 æŸ¥çœ‹ç»“æœã€‚",
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tokenizer.Feed(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Feed() = %v, want %v", got, tt.expected)
			}
		})
	}
}

func TestRuleBoundaryTokenizer_Abbreviations(t *testing.T) {
	tokenizer := NewRuleBoundaryTokenizer(nil)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	tests := []struct {
		name     string
		input    string
		expected []string
	}{
		{
			name:  "è‹±æ–‡ç¼©ç•¥è¯",
			input: "Mr. Smith visited Dr. Johnson at 9 a.m. Then he went home.",
			expected: []string{
				"Mr. Smith visited Dr. Johnson at 9 a.m.",
				"Then he went home.",
			},
		},
		{
			name:  "å¤šä¸ªç¼©ç•¥è¯",
			input: "Prof. Lee and Dr. Wang from the U.S.A. visited us.",
			expected: []string{
				"Prof. Lee and Dr. Wang from the U.S.A. visited us.",
			},
		},
		{
			name:  "æ—¶é—´ç¼©ç•¥è¯",
			input: "The meeting is at 9 a.m. The lunch is at 12 p.m. Sharp!",
			expected: []string{
				"The meeting is at 9 a.m.",
				"The lunch is at 12 p.m. Sharp!",
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tokenizer.Feed(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Feed() = %v, want %v", got, tt.expected)
			}
		})
	}
}

func TestRuleBoundaryTokenizer_QuotedText(t *testing.T) {
	tokenizer := NewRuleBoundaryTokenizer(nil)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	tests := []struct {
		name     string
		input    string
		expected []string
	}{
		{
			name:  "è‹±æ–‡å¼•å·",
			input: `He said: "This is great. I love it." Then he left.`,
			expected: []string{
				`He said: "This is great. I love it." Then he left.`,
			},
		},
		{
			name:  "ä¸­æ–‡å¼•å·",
			input: `ä»–è¯´ï¼š"ä»Šå¤©å¤©æ°”çœŸå¥½ã€‚æ˜å¤©ä¹Ÿä¼šå¾ˆå¥½ã€‚"ç„¶åç¦»å¼€äº†ã€‚`,
			expected: []string{
				`ä»–è¯´ï¼š"ä»Šå¤©å¤©æ°”çœŸå¥½ã€‚æ˜å¤©ä¹Ÿä¼šå¾ˆå¥½ã€‚"ç„¶åç¦»å¼€äº†ã€‚`,
			},
		},
		{
			name:  "åµŒå¥—å¼•å·",
			input: `ä»–è¯´ï¼š"å°æ˜å‘Šè¯‰æˆ‘'ä»Šå¤©å¤©æ°”çœŸå¥½'ï¼Œæˆ‘ä¹Ÿè¿™ä¹ˆè§‰å¾—ã€‚"`,
			expected: []string{
				`ä»–è¯´ï¼š"å°æ˜å‘Šè¯‰æˆ‘'ä»Šå¤©å¤©æ°”çœŸå¥½'ï¼Œæˆ‘ä¹Ÿè¿™ä¹ˆè§‰å¾—ã€‚"`,
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tokenizer.Feed(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Feed() = %v, want %v", got, tt.expected)
			}
		})
	}
}

func TestRuleBoundaryTokenizer_StreamingInput(t *testing.T) {
	tokenizer := NewRuleBoundaryTokenizer(nil)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	input := "ç¬¬ä¸€å¥è¯ã€‚ç¬¬äºŒå¥è¯ã€‚"
	expected := []string{
		"ç¬¬ä¸€å¥è¯ã€‚",
		"ç¬¬äºŒå¥è¯ã€‚",
	}

	var got []string
	for _, char := range input {
		if sentences := tokenizer.FeedChar(char); len(sentences) > 0 {
			got = append(got, sentences...)
		}
	}

	if !reflect.DeepEqual(got, expected) {
		t.Errorf("Streaming input result = %v, want %v", got, expected)
	}
}

func TestRuleBoundaryTokenizer_LongSentences(t *testing.T) {
	config := &RuleBoundaryConfig{
		MinLength:     1,
		MaxLength:     15,
		TrimSpace:     true,
		MergeNewlines: true,
		KeepEmpty:     false,
		EnableCJK:     true,
		EnableWestern: true,
	}

	tokenizer := NewRuleBoundaryTokenizer(config)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	tests := []struct {
		name     string
		input    string
		expected []string
	}{
		{
			name:  "é•¿å¥å­åˆ†å‰²",
			input: "è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸éå¸¸éå¸¸éå¸¸éå¸¸éå¸¸éå¸¸é•¿çš„å¥å­ï¼Œéœ€è¦è¢«åˆ†å‰²ã€‚",
			expected: []string{
				"è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸éå¸¸",
				"éå¸¸éå¸¸éå¸¸éå¸¸",
				"éå¸¸é•¿çš„å¥å­ï¼Œ",
				"éœ€è¦è¢«åˆ†å‰²ã€‚",
			},
		},
		{
			name:  "è‹±æ–‡é•¿å¥å­",
			input: "This is a very very very very very very very very long sentence that needs to be split.",
			expected: []string{
				"This is a very",
				"very very very",
				"very very very",
				"very long",
				"sentence that",
				"needs to be",
				"split.",
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tokenizer.Feed(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Feed() = %v, want %v", got, tt.expected)
			}
		})
	}
}

func TestRuleBoundaryTokenizer_EmptyLines(t *testing.T) {
	config := &RuleBoundaryConfig{
		MinLength:     1,
		MaxLength:     1000,
		TrimSpace:     true,
		MergeNewlines: false,
		KeepEmpty:     true,
	}

	tokenizer := NewRuleBoundaryTokenizer(config)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	tests := []struct {
		name     string
		input    string
		expected []string
	}{
		{
			name:  "ç©ºè¡Œä¿ç•™",
			input: "ç¬¬ä¸€è¡Œã€‚\n\nç¬¬äºŒè¡Œã€‚",
			expected: []string{
				"ç¬¬ä¸€è¡Œã€‚",
				"",
				"ç¬¬äºŒè¡Œã€‚",
			},
		},
		{
			name:  "å¤šä¸ªç©ºè¡Œ",
			input: "ç¬¬ä¸€æ®µã€‚\n\n\nç¬¬äºŒæ®µã€‚",
			expected: []string{
				"ç¬¬ä¸€æ®µã€‚",
				"",
				"",
				"ç¬¬äºŒæ®µã€‚",
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tokenizer.Feed(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Feed() = %v, want %v", got, tt.expected)
			}
		})
	}
}

func TestRuleBoundaryTokenizer_Reset(t *testing.T) {
	tokenizer := NewRuleBoundaryTokenizer(nil)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	// ç¬¬ä¸€æ¬¡è¾“å…¥
	input1 := "ç¬¬ä¸€å¥ã€‚"
	got1 := tokenizer.Feed(input1)
	expected1 := []string{"ç¬¬ä¸€å¥ã€‚"}

	if !reflect.DeepEqual(got1, expected1) {
		t.Errorf("First feed = %v, want %v", got1, expected1)
	}

	// é‡ç½®
	tokenizer.Reset()

	// ç¬¬äºŒæ¬¡è¾“å…¥
	input2 := "ç¬¬äºŒå¥ã€‚"
	got2 := tokenizer.Feed(input2)
	expected2 := []string{"ç¬¬äºŒå¥ã€‚"}

	if !reflect.DeepEqual(got2, expected2) {
		t.Errorf("Second feed after reset = %v, want %v", got2, expected2)
	}
}

func TestRuleBoundaryTokenizer_CustomConfig(t *testing.T) {
	config := &RuleBoundaryConfig{
		MinLength:      5,
		MaxLength:      50,
		TrimSpace:      true,
		MergeNewlines:  true,
		KeepEmpty:      false,
		EnableCJK:      true,
		EnableWestern:  true,
		EnableEmoji:    true,
		EnableURLs:     true,
		EnableNumbers:  true,
		EnableQuotes:   true,
		EnableEllipsis: true,
	}

	tokenizer := NewRuleBoundaryTokenizer(config)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	tests := []struct {
		name     string
		input    string
		expected []string
	}{
		{
			name:  "æœ€å°é•¿åº¦è¿‡æ»¤",
			input: "çŸ­ã€‚è¿™æ˜¯ä¸€ä¸ªé•¿åº¦è¶…è¿‡äº”ä¸ªå­—çš„å¥å­ã€‚",
			expected: []string{
				"è¿™æ˜¯ä¸€ä¸ªé•¿åº¦è¶…è¿‡äº”ä¸ªå­—çš„å¥å­ã€‚",
			},
		},
		{
			name:  "æ•°å­—å’Œè¡¨æƒ…ç¬¦å·",
			input: "ä»·æ ¼æ˜¯99.9%ã€‚ğŸ˜Šè¿™çœŸæ˜¯å¤ªå¥½äº†ï¼",
			expected: []string{
				"ä»·æ ¼æ˜¯99.9%ã€‚",
				"ğŸ˜Šè¿™çœŸæ˜¯å¤ªå¥½äº†ï¼",
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tokenizer.Feed(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Feed() = %v, want %v", got, tt.expected)
			}
		})
	}
}

func TestRuleBoundaryTokenizer_ConfigClone(t *testing.T) {
	config := &RuleBoundaryConfig{
		MinLength:     5,
		MaxLength:     50,
		TrimSpace:     true,
		MergeNewlines: true,
		KeepEmpty:     false,
	}

	tokenizer := NewRuleBoundaryTokenizer(config)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	// è·å–é…ç½®å¹¶ä¿®æ”¹
	currentConfig := tokenizer.Config()
	clonedConfig := currentConfig.Clone()

	// ä¿®æ”¹åŸå§‹é…ç½®
	config.MinLength = 10
	config.MaxLength = 100

	// éªŒè¯å…‹éš†çš„é…ç½®æ²¡æœ‰è¢«ä¿®æ”¹
	if cfg, ok := clonedConfig.(*RuleBoundaryConfig); ok {
		if cfg.MinLength != 5 || cfg.MaxLength != 50 {
			t.Errorf("Cloned config was modified: got MinLength=%d, MaxLength=%d, want MinLength=5, MaxLength=50",
				cfg.MinLength, cfg.MaxLength)
		}
	} else {
		t.Error("Failed to cast cloned config to RuleBoundaryConfig")
	}
}

// invalidConfig ç”¨äºæµ‹è¯•æ— æ•ˆé…ç½®
type invalidConfig struct{}

func (c *invalidConfig) Clone() TokenizerConfig { return c }

func TestRuleBoundaryTokenizer_InvalidConfig(t *testing.T) {
	tokenizer := NewRuleBoundaryTokenizer(nil)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	// å°è¯•è®¾ç½®æ— æ•ˆé…ç½®
	err := tokenizer.SetConfig(&invalidConfig{})
	if err != ErrInvalidConfig {
		t.Errorf("SetConfig() with invalid config = %v, want %v", err, ErrInvalidConfig)
	}
}

func TestRuleBoundaryTokenizer_Numbers(t *testing.T) {
	config := &RuleBoundaryConfig{
		MinLength:     1,
		MaxLength:     1000,
		TrimSpace:     true,
		MergeNewlines: true,
		KeepEmpty:     false,
		EnableCJK:     true,
		EnableWestern: true,
		EnableNumbers: true,
	}

	tokenizer := NewRuleBoundaryTokenizer(config)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	tests := []struct {
		name     string
		input    string
		expected []string
	}{
		{
			name:  "ç®€å•æ•°å­—",
			input: "ä»·æ ¼æ˜¯ 123.45 å…ƒã€‚ä¸‹ä¸€å¥å¼€å§‹ã€‚",
			expected: []string{
				"ä»·æ ¼æ˜¯ 123.45 å…ƒã€‚",
				"ä¸‹ä¸€å¥å¼€å§‹ã€‚",
			},
		},
		{
			name:  "å¤šä¸ªæ•°å­—",
			input: "ç¬¬ä¸€ä¸ªæ•°å­—æ˜¯ 1.23ï¼Œç¬¬äºŒä¸ªæ•°å­—æ˜¯ 4.56ã€‚",
			expected: []string{
				"ç¬¬ä¸€ä¸ªæ•°å­—æ˜¯ 1.23ï¼Œç¬¬äºŒä¸ªæ•°å­—æ˜¯ 4.56ã€‚",
			},
		},
		{
			name:  "å¸¦åƒä½åˆ†éš”ç¬¦çš„æ•°å­—",
			input: "æ€»é‡‘é¢ä¸º 1,234,567.89 å…ƒã€‚ç»“æŸã€‚",
			expected: []string{
				"æ€»é‡‘é¢ä¸º 1,234,567.89 å…ƒã€‚",
				"ç»“æŸã€‚",
			},
		},
		{
			name:  "ç§‘å­¦è®¡æ•°æ³•",
			input: "æ•°å€¼ä¸º 1.23e-4 å•ä½ã€‚ç»§ç»­ã€‚",
			expected: []string{
				"æ•°å€¼ä¸º 1.23e-4 å•ä½ã€‚",
				"ç»§ç»­ã€‚",
			},
		},
		{
			name:  "ç™¾åˆ†æ¯”",
			input: "å¢é•¿ç‡ä¸º 12.34%ã€‚ä¸‹ä¸€å¥ã€‚",
			expected: []string{
				"å¢é•¿ç‡ä¸º 12.34%ã€‚",
				"ä¸‹ä¸€å¥ã€‚",
			},
		},
		{
			name:  "æ··åˆæ•°å­—å’Œæ–‡æœ¬",
			input: "ç‰ˆæœ¬å· 2.0.1 å‘å¸ƒäº†ã€‚è¿™æ˜¯æ–°ç‰ˆæœ¬ã€‚",
			expected: []string{
				"ç‰ˆæœ¬å· 2.0.1 å‘å¸ƒäº†ã€‚",
				"è¿™æ˜¯æ–°ç‰ˆæœ¬ã€‚",
			},
		},
		{
			name:  "IPåœ°å€",
			input: "æœåŠ¡å™¨IPæ˜¯ 192.168.1.1ã€‚ç»§ç»­ã€‚",
			expected: []string{
				"æœåŠ¡å™¨IPæ˜¯ 192.168.1.1ã€‚",
				"ç»§ç»­ã€‚",
			},
		},
		{
			name:  "æ•°å­—èŒƒå›´",
			input: "æ¸©åº¦åœ¨ 36.5-37.2 åº¦ä¹‹é—´ã€‚æ­£å¸¸ã€‚",
			expected: []string{
				"æ¸©åº¦åœ¨ 36.5-37.2 åº¦ä¹‹é—´ã€‚",
				"æ­£å¸¸ã€‚",
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tokenizer.Feed(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Feed() = %v, want %v", got, tt.expected)
			}
		})
	}
}

func TestRuleBoundaryTokenizer_DisabledNumbers(t *testing.T) {
	config := &RuleBoundaryConfig{
		MinLength:     1,
		MaxLength:     1000,
		TrimSpace:     true,
		MergeNewlines: true,
		KeepEmpty:     false,
		EnableCJK:     true,
		EnableWestern: true,
		EnableNumbers: false, // ç¦ç”¨æ•°å­—å¤„ç†
	}

	tokenizer := NewRuleBoundaryTokenizer(config)
	if err := tokenizer.Init(context.Background()); err != nil {
		t.Fatalf("Failed to init tokenizer: %v", err)
	}
	defer tokenizer.Close()

	tests := []struct {
		name     string
		input    string
		expected []string
	}{
		{
			name:  "ç¦ç”¨æ•°å­—å¤„ç†æ—¶çš„å°æ•°",
			input: "ä»·æ ¼æ˜¯ 123.45 å…ƒã€‚",
			expected: []string{
				"ä»·æ ¼æ˜¯ 123.",
				"45 å…ƒã€‚",
			},
		},
		{
			name:  "ç¦ç”¨æ•°å­—å¤„ç†æ—¶çš„IPåœ°å€",
			input: "IPæ˜¯ 192.168.1.1ã€‚",
			expected: []string{
				"IPæ˜¯ 192.",
				"168.",
				"1.",
				"1ã€‚",
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tokenizer.Feed(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Feed() = %v, want %v", got, tt.expected)
			}
		})
	}
}
